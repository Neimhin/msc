#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\rightmargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Optimization 8, Neimhin Robinson Gunning, 16321701
\end_layout

\begin_layout Section*
(a)
\end_layout

\begin_layout Subsection*
(a) (i)
\end_layout

\begin_layout Standard
An implementation of global random search is provided in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{src/global
\backslash
_random
\backslash
_search.py}
\end_layout

\end_inset

 in the appendix.
 The implementation allows specifying different upper and lower bounds for
 each parameter when sampling random parameter vectors.
\end_layout

\begin_layout Subsection*
(a) (ii)
\end_layout

\begin_layout Standard
Here we compare the performance of global random search and gradient descent
 with constant step size on two functions, 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(\boldsymbol{x})=3(x_{1}-5)^{4}+10(x_{2}-9)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(\boldsymbol{x})=\max(x_{1}-5,0)+10|x_{2}-9|
\]

\end_inset


\end_layout

\begin_layout Standard
We can note that is 
\emph on
possible
\emph default
 for the global random search to perform wildly better than gradient descent
 if the first random parameter vector is the global optimum, however, the
 performance does tend to follow consistent patterns.
 It's not fair to compare performance with respect to number of iterations
 because the global random search algorithm can perform many more (about
 100 times more in this case) iterations than gradient descent in the same
 wall-clock time.
 The performance w.r.t.
 iterations in the top two plots of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:aii-perf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 seems pessimistic about global random search.
 We compare the wall-clock performance in the bottom two plots.
 For 
\begin_inset Formula $f$
\end_inset

 the algorithms have roughly comparable performance.
 In one case global random search algorithm achieves the lowest loss.
 For 
\begin_inset Formula $g$
\end_inset

 (which has non-smoothness and regions with constant gradients), we are
 able to find a step size (0.003) which causes gradient descent to consistently
 perform much better than global random search.
 For GRS in all cases here we set 
\begin_inset Formula $l_{1}=0,u_{1}=10$
\end_inset

 and 
\begin_inset Formula $l_{2}=0,u_{2}=18$
\end_inset

, which puts the global minimum (5,9), at the center of these two ranges.
 The question of tuning these bounds is tricky, because obviously the best
 values would be 
\begin_inset Formula $l_{1}=5,u_{1}=5$
\end_inset

 and 
\begin_inset Formula $l_{2}=9,u_{2}=9$
\end_inset

, but it is not fair to encode this prior knowledge when considering how
 the algorithm would behave for functions with less known behaviour.
\end_layout

\begin_layout Standard
We find that global random search performs about 100 times more function
 evaluations per second than the gradient descent algorithm.
 Comparing the wall-clock performance is difficult here for two reasons.
 For one, we need to keep track of the cost function values for each parameter
 estimate of gradient descent, which adds memory and compute overhead to
 gradient descent which would not be necessary if we were running the algorithm
 without diagnostics.
 The second reason is that global random search is almost arbitrarily parralleli
zable, so we could, given enough cores, bring the global random search algorithm
 down to constant time, give or take some coordination.
 For this reason in subsequent sections we will include comparisons based
 on the number of function or gradient evaluations required by the algorithm.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/aii-iterations-f.pdf
	lyxscale 40
	width 45text%

\end_inset


\begin_inset Graphics
	filename fig/aii-iterations-g.pdf
	lyxscale 40
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/aii-time-f.pdf
	width 45text%

\end_inset


\begin_inset Graphics
	filename fig/aii-time-g.pdf
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:aii-perf"

\end_inset

Global random search vs gradient descent in terms of both iterations and
 wall-clock time.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(b) (i)
\end_layout

\begin_layout Standard
We now modify the above global random search algorithm in two ways.
 We call the first modification 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b}
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection*
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
Initialization Phase: Same as (a) set 
\begin_inset Formula $l_{i}$
\end_inset

 and 
\begin_inset Formula $u_{i}$
\end_inset

 for each parameter 
\begin_inset Formula $i$
\end_inset

.
 Then generate 
\begin_inset Formula $N$
\end_inset

 random parameter vectors and evaluate the cost function for each.
 Select the 
\begin_inset Formula $M$
\end_inset

 parameter vectors with the lowest cost, call these 
\begin_inset Formula $B_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
Iteration Phase: Randomly select a parameter vector 
\begin_inset Formula $p$
\end_inset

 from the 
\begin_inset Formula $M$
\end_inset

 selected parameter vectors.
 Generate a new parameter vector by perturbing each 
\begin_inset Formula $p_{i}$
\end_inset

 by 
\begin_inset Formula $\delta\sim\mathcal{U}(-\alpha(u_{i}-l_{i}),\alpha(u_{i}-l_{i}))$
\end_inset

, where 
\begin_inset Formula $\alpha$
\end_inset

 is a new hyperparameter we have to set.
 Repeat until 
\begin_inset Formula $N$
\end_inset

 paramater vectors have been generated.
 Evaluate the cost function at each of the 
\begin_inset Formula $N$
\end_inset

 paramater vectors and select 
\begin_inset Formula $b_{i}$
\end_inset

 the 
\begin_inset Formula $M$
\end_inset

 best of these.
 Join the new best 
\begin_inset Formula $M$
\end_inset

, 
\begin_inset Formula $b_{i}$
\end_inset

, with the old best 
\begin_inset Formula $M$
\end_inset

, 
\begin_inset Formula $B_{i-1}$
\end_inset

 and select the best 
\begin_inset Formula $M$
\end_inset

 of 
\begin_inset Formula $b_{i}\cup B_{i-1}$
\end_inset

 to get 
\begin_inset Formula $B_{i}$
\end_inset

.
 Repeat the iteration phase.
\end_layout

\begin_layout Subsubsection*
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b
\backslash
_mod}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
We call the second modified algorithm 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b
\backslash
_mod}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Initialization: Same as (a) set 
\begin_inset Formula $l_{i}$
\end_inset

 and 
\begin_inset Formula $u_{i}$
\end_inset

 for each parameter 
\begin_inset Formula $i$
\end_inset

 manully.
\end_layout

\begin_layout Standard
Iteration Phase: Then generate 
\begin_inset Formula $N$
\end_inset

 random parameter vectors and evaluate the cost function for each.
 Select the 
\begin_inset Formula $M$
\end_inset

 parameter vectors with the lowest cost.
 Now we update all 
\begin_inset Formula $l_{i}$
\end_inset

 and 
\begin_inset Formula $u_{i}$
\end_inset

 based on the 
\begin_inset Formula $M$
\end_inset

 selected parameter vectors, 
\begin_inset Formula $x^{1},x^{2},...,x^{M}$
\end_inset

.
 We set 
\begin_inset Formula $l_{i}:=\min(x_{i}^{1},x_{i}^{2},...,x_{i}^{M})$
\end_inset

 and 
\begin_inset Formula $u_{i}:=\max(x_{i}^{1},x_{i}^{2},...,x_{i}^{M})$
\end_inset

 for each 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Standard
We compare the performance of the three algorithms 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

, and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bii-perf"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection*
(b) (ii)
\end_layout

\begin_layout Standard
Here we compare the performance of the three described random search algorithms.
 The top two plots of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bii-perf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 compare cost as a function of wall-clock time.
 We use 
\begin_inset Formula $l_{1}=0,u_{1}=10$
\end_inset

 and 
\begin_inset Formula $l_{2}=0,u_{2}=18$
\end_inset

 in all cases.
 The gradient descent step size is tuned to each function and starts at
 
\begin_inset Formula $x=[0,0]$
\end_inset

.
 For 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b}
\end_layout

\end_inset

 we use 
\begin_inset Formula $N=400$
\end_inset

, 
\begin_inset Formula $M=100$
\end_inset

 and 
\begin_inset Formula $\alpha=0.001$
\end_inset

.
 For 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b
\backslash
_mod}
\end_layout

\end_inset

 we use 
\begin_inset Formula $N=100$
\end_inset

 and 
\begin_inset Formula $M=10$
\end_inset

.
\end_layout

\begin_layout Standard
For both functions 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

, the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b
\backslash
_mod}
\end_layout

\end_inset

 algorithm is by far the fastest to converge, and tends to actually find
 the global minimum, 
\begin_inset Formula $x=[5,9]$
\end_inset

.
 However, it is possible for the algorithm to diverge, which is seen in
 some cases in the bottom left plot of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bii-perf"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 This occurs when the min and max for each parameter value 
\begin_inset Formula $l_{i}$
\end_inset

 and 
\begin_inset Formula $u_{i}$
\end_inset

 defines an interval that does not include the global optimum.
 The choices of 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 are important in terms of probability that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b
\backslash
_mod}
\end_layout

\end_inset

 will diverge.
 If 
\begin_inset Formula $N$
\end_inset

 is too low then there is a higher probability of randomly choosing paramaters
 `on one side' of the global minimum, which will cause the new min and max
 values to lie outside the global minimum.
 A similar argument applies to the size of 
\begin_inset Formula $M$
\end_inset

.
 With low 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 we have a chance of converging much faster, but also a higher chance of
 diverging.
 With large 
\begin_inset Formula $N$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 we take a safer but probably slower route.
\end_layout

\begin_layout Standard
The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search a}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 algorithms have more comparable performance, but by tuning 
\begin_inset Formula $\alpha$
\end_inset

 (I won't discuss the details of tuning 
\begin_inset Formula $\alpha$
\end_inset

) we have found a configuration in which 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{rnd search b}
\end_layout

\end_inset

 usually performs a little bit better than 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

 on both functions 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

, given sufficient iterations.
 The initial convergence speed of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 is sometimes quite poor because it depends heavily on the initial sample
 of parameter vectors.
 However, the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 algorithm has the property that it can continue to make slow and steady
 progress, even if it ends up in a state where 
\begin_inset Formula $B_{i}$
\end_inset

 consists only of bad parameter vectors.
 An example of this can be seen in the second row, first column of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:contours"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 After a few iterations all subsequent samples from the parameter space
 are limited to a small region defined by 
\begin_inset Formula $B_{i}$
\end_inset

, which is far from the optimum.
\end_layout

\begin_layout Standard
The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

 algorithm is plauged by diminishing returns, whenever it makes progress
 the chances of further progress are diminished.
\end_layout

\begin_layout Standard
In Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:contours"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we plot the random guesses of a run of each algorithm superimposed on each
 function, with 
\begin_inset Formula $N=100$
\end_inset

, 
\begin_inset Formula $M=50$
\end_inset

 for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

, and 
\begin_inset Formula $\alpha=0.01$
\end_inset

.
 For 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

 we use 
\begin_inset Formula $N=300$
\end_inset

.
 
\end_layout

\begin_layout Standard
.
 The left column has 
\begin_inset Formula $f$
\end_inset

 the right colum has 
\begin_inset Formula $g$
\end_inset

.
 We represent time with the hue of the points, lighter (whiter) being earlier
 and darker (bluer) being later.
 We plot every subsequent sample, regardless of whether it yielded the lowest
 cost seen in the run thus far.
 In the top row (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

) we see that the samples are independent of time, resembling the bivariate
 uniform.
 The behaviour of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 in the second row starts out somewhat uniform (white), then samples become
 concentrated to a handful of disjoint regions (light blue), until finally
 all samples are being taken from one small region (dark blue).
 The third row (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

) shows that early samples are uniform and from a large rectangle, then
 a smaller rectangle, until eventually all samples are from a very thin
 rectangle encompassing the global minimum.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bii-time-f.pdf
	lyxscale 200
	width 45text%

\end_inset


\begin_inset Graphics
	filename fig/bii-time-g.pdf
	lyxscale 200
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bii-evals-f.pdf
	lyxscale 200
	width 45text%

\end_inset


\begin_inset Graphics
	filename fig/bii-evals-g.pdf
	lyxscale 200
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:bii-perf"

\end_inset

Comparison of the three proposed random search algorithms, top: time vs
 cost, bottom: evals vs cost.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename bii/fa.pdf
	width 30text%

\end_inset


\begin_inset Graphics
	filename bii/ga.pdf
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename bii/fb.pdf
	width 30text%

\end_inset


\begin_inset Graphics
	filename bii/gb.pdf
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename bii/fb_mod.pdf
	width 30text%

\end_inset


\begin_inset Graphics
	filename bii/gb_mod.pdf
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:contours"

\end_inset

Progressive guesses made by each of the algorithms, superimposed onto the
 contour plots of the functions 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

.
 White dots are earlier guesses, darker blue dots are later guesses.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(c)
\end_layout

\begin_layout Standard
Here we use all three algorithms to tune the hyperparameters of CNN, which
 we are training in a 10 class classification problem.
 The hyperparameters are (i) minibatch size, (ii) adam optimizer params
 (
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\beta_{1}$
\end_inset

, 
\begin_inset Formula $\beta_{2}$
\end_inset

), and (iii) number of epochs, i.e.
 there are 5 hyper parameters.
 Our cost function 
\begin_inset Formula $c(x)$
\end_inset

 takes a parameter vector 
\begin_inset Formula $x\in\mathbb{R}^{5}$
\end_inset

, where the 
\begin_inset Formula $x_{i}$
\end_inset

s are the respective hyperparameters.
 The cost function trains a CNN with those hyperparameters, on 
\begin_inset Formula $n=1000$
\end_inset

 examples from the training set, and then evaluates the model on 
\begin_inset Formula $10,000$
\end_inset

 examples from the test set, returning the categorical cross entropy loss
 of the model on the test set.
 The min and max for each parameter are presented in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:min-and-max"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It is an expensive function, especially when the number of epochs is high.
 As a baseline we train the model with 
\begin_inset Formula $\text{\texttt{minibatch\_size}=128, \ensuremath{\alpha=0.001},}\beta_{1}=0.9,\beta_{2}=0.999$
\end_inset

 and 
\begin_inset Formula $\texttt{{num\_epochs}=40}$
\end_inset

, which achieves a loss of 1.8646.
\end_layout

\begin_layout Standard
Each algorithm is given 99 function evaluations (for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

 
\begin_inset Formula $N=99$
\end_inset

; for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 
\begin_inset Formula $N=33,M=10$
\end_inset

 and we run three iterations).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename src/cps.py.pdf
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:min-and-max"

\end_inset

Min and max parameter values to initialize each random search algorithm.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We present the value of the cost function over the number of function evaluation
s for each algorithm in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:c-cost"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Alg 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 both descends most quickly and ends up with the lowest final cost.
 Alg 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

 starts out more promising than 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 but ultimately 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 has a lower final cost than 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/c.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:c-cost"

\end_inset

Training cost vs function evaluations for the CNN using three algs.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The final parameter values selected by each algorithm are presented in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:best-params"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We now train 3 models with the parameters presented in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:best-params"
plural "false"
caps "false"
noprefix "false"

\end_inset

, using the same 
\begin_inset Formula $1000$
\end_inset

 training samples, but evaluating on the categorical cross-entropy on a
 held test set of 
\begin_inset Formula $49,000$
\end_inset

 samples.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename a-N99.json.pdf
	width 33text%

\end_inset


\begin_inset Graphics
	filename b-N33-M10-it3.json.pdf
	width 33text%

\end_inset


\begin_inset Graphics
	filename b_mod-N33-M10-it3.json.pdf
	width 33text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:best-params"

\end_inset

Best params, and best training cost for each alg on the CNN task.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
While 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 was fastest to reduce the loss and reached the lowest ultimate training
 loss, we find that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 has the best ultimate performance on the held-out test set (see Table
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:c-test-cost"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 We suspect the reason 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 didn't perform as well on the held-out test set, is because of that algorithms
 ability to continue exploring indefinitely, likely leading to overfitting
 the hyperparameters.
 What happens if we stop 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 early instead? Using the parameters from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset

 after 5 function evaluations we get a better loss on the held-out test
 set of 1.8038, but still not as good as 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

.
 These results are tentative because we have only run each algorithm once.
 However, an explanation as to why 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset

 might be ultimately better is that it converges monotonically to smaller
 parameter spaces, preventing excessive exploration and therefore preventing
 overfitting.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{a}
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b}
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{b
\backslash
_mod}
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
held-out loss
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.998
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.8337
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.790
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:c-test-cost"

\end_inset

Ultimate categorical cross-entropy losses on the held-out test set (
\begin_inset Formula $n=49,000$
\end_inset

) for each random search alg.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
