#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Optimisation Algorithms for Machine Learning
\end_layout

\begin_layout Title
Week 6 Assignment
\end_layout

\begin_layout Author
Neimhin Robinson Gunning, 16321701
\end_layout

\begin_layout Date
28th March 2024
\end_layout

\begin_layout Standard

\series bold
(a) (i) Implementing mini-batch Stochastic Gradient Descent
\end_layout

\begin_layout Standard
Our global loss function is
\begin_inset Formula 
\[
f_{T}(x)=\sum_{w\in T}\frac{loss(x,w)}{\#W}
\]

\end_inset

which is just the average of 
\begin_inset Formula $loss(w,x)$
\end_inset

 ranging over the entire dataset, 
\begin_inset Formula $T$
\end_inset

.
 We can also calculate an approximation of the loss using a subset (minibatch)
 of 
\begin_inset Formula $T$
\end_inset

.
\begin_inset Formula 
\[
f_{N}(x)=\sum_{w\in N}\frac{loss(x,w)}{\#N}
\]

\end_inset

This is implemented on line 17 of Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We can also approximate the gradient w.r.t.
 to the minibatch rather than the full training dataset.
 In these experiments we use the finite difference methods to estimate the
 mini-batch gradient according to
\begin_inset Formula 
\[
\frac{df_{N}}{dx_{i}}\approx\frac{f_{N}([x_{1},\ldots,x_{i}+\epsilon,\ldots,x_{d}])-f_{N}(x)}{\epsilon}
\]

\end_inset

where we set 
\begin_inset Formula $\epsilon=10^{-15}$
\end_inset

 for the remainder of this discussion.
 We also look at only at an example with 
\begin_inset Formula $d=2$
\end_inset

 so the finite difference gradient is:
\begin_inset Formula 
\[
\nabla f_{N}=[\frac{f_{N}([x_{1}+\epsilon,x_{2}])-f_{N}(x)}{\epsilon},\frac{f_{N}([x_{1},x_{2}+\epsilon])-f_{N}(x)}{\epsilon}]
\]

\end_inset

The code implementation of this is on line 4 in Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To generate mini-batches we first shuffle the rows data set and then take
 successive slices with 
\begin_inset Formula $n$
\end_inset

 rows, where 
\begin_inset Formula $n$
\end_inset

 is the mini-batch size.
\end_layout

\begin_layout Standard
The implementation of mini-batch SGD here relies on generating successive
 
\begin_inset Formula $f_{N,t}$
\end_inset

 and 
\begin_inset Formula $\nabla f_{N,t}$
\end_inset

, where 
\begin_inset Formula $t$
\end_inset

 is the iteration number.
 At each iteration the step size can be calculated with any of the Polyak,
 RMSProp, Heavy Ball, and Adam methods.
 Each of the step types are implemented in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{src/sgd.py}
\end_layout

\end_inset

 which is included in the appendix.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/ai-code.pdf
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:minibatch-sgd"

\end_inset

Generating mini-batches, 
\begin_inset Formula $N$
\end_inset

, and associated 
\begin_inset Formula $f_{N}$
\end_inset

 and 
\begin_inset Formula $\nabla f_{N}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
