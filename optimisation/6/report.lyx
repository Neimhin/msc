#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Optimisation Algorithms for Machine Learning
\end_layout

\begin_layout Title
Week 6 Assignment
\end_layout

\begin_layout Author
Neimhin Robinson Gunning, 16321701
\end_layout

\begin_layout Date
28th March 2024
\end_layout

\begin_layout Standard

\series bold
(a) (i) Implementing mini-batch Stochastic Gradient Descent
\end_layout

\begin_layout Standard
Our global loss function is
\begin_inset Formula 
\[
f_{T}(x)=\sum_{w\in T}\frac{loss(x,w)}{\#W}
\]

\end_inset

which is just the average of 
\begin_inset Formula $loss(w,x)$
\end_inset

 ranging over the entire dataset, 
\begin_inset Formula $T$
\end_inset

.
 We can also calculate an approximation of the loss using a subset (minibatch)
 of 
\begin_inset Formula $T$
\end_inset

.
\begin_inset Formula 
\[
f_{N}(x)=\sum_{w\in N}\frac{loss(x,w)}{\#N}
\]

\end_inset

This is implemented on line 17 of Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We can also approximate the gradient w.r.t.
 to the minibatch rather than the full training dataset.
\end_layout

\begin_layout Standard
To generate mini-batches we first shuffle the rows data set and then take
 successive slices with 
\begin_inset Formula $n$
\end_inset

 rows, where 
\begin_inset Formula $n$
\end_inset

 is the mini-batch size.
 The first mini-batch consists of the 1st to the 
\begin_inset Formula $n$
\end_inset

th data items, the second consists of the 
\begin_inset Formula $(n+1)th$
\end_inset

 to the 
\begin_inset Formula $(n+n)th$
\end_inset

, etc.
 If we reach the end of the dataset before filling the minibatch we shuffle
 the dataset and start again from index 1.
 This is implemented on line 10 of Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
The implementation of mini-batch SGD here relies on generating successive
 
\begin_inset Formula $f_{N_{t}}$
\end_inset

 and 
\begin_inset Formula $\nabla f_{N_{t}}$
\end_inset

, where 
\begin_inset Formula $N_{t}$
\end_inset

 is the mini-batch for iteration 
\begin_inset Formula $t$
\end_inset

.
 This is implemented on line 40 of Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
At each iteration the step size can be calculated with respect to 
\begin_inset Formula $f_{N_{t}}$
\end_inset

 and 
\begin_inset Formula $\nabla f_{N_{t}}$
\end_inset

 using of the Polyak, RMSProp, Heavy Ball, and Adam methods.
 Each of the step types are implemented in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{src/sgd.py}
\end_layout

\end_inset

 which is included in the appendix.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/ai-code.pdf
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:minibatch-sgd"

\end_inset

Generating mini-batches, 
\begin_inset Formula $N$
\end_inset

, and associated 
\begin_inset Formula $f_{N}$
\end_inset

 and 
\begin_inset Formula $\nabla f_{N}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
(a) (ii)
\end_layout

\begin_layout Standard
I downloaded a pair of functions which are reproduced in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:function-downloaded"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 On line 6 is a python definition of 
\begin_inset Formula $f_{N}(x)$
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 corresponds to 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{minibatch}
\end_layout

\end_inset

.
 We generate 
\begin_inset Formula $T$
\end_inset

 using lines 3 and 4 of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:function-downloaded"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and use the same 
\begin_inset Formula $T$
\end_inset

 throughout the remainder of the discussion.
 A wireframe and a contour plot of 
\begin_inset Formula $f_{T}(X)$
\end_inset

 is presented in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wireframe-and-contour"
plural "false"
caps "false"
noprefix "false"

\end_inset

, showing 
\begin_inset Formula $x\in[-5,5]^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename funcs.pdf
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:function-downloaded"

\end_inset

Functions downloaded for this assignment from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
url{https://www.scss.tcd.ie/Doug.Leith/CS7DS2/week6.php}
\end_layout

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig/wire-contour.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:wireframe-and-contour"

\end_inset

Wireframe (left) and contour (right) plots of 
\begin_inset Formula $f_{T}(x)$
\end_inset

 where 
\begin_inset Formula $T$
\end_inset

 is a `training set' generated by the code in lines 3 and 4 of Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:function-downloaded"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
(a) (iii) Gradient Calculation
\end_layout

\begin_layout Standard
Implementations of both analytic calculation of gradients with sympy and
 finite difference are provided in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{src/week6.py}
\end_layout

\end_inset

.
 In these experiments we use the finite difference methods to estimate the
 mini-batch gradient according to
\begin_inset Formula 
\[
\frac{df_{N}}{dx_{i}}\approx\frac{f_{N}([x_{1},\ldots,x_{i}+\epsilon,\ldots,x_{d}])-f_{N}(x)}{\epsilon}
\]

\end_inset

where we set 
\begin_inset Formula $\epsilon=10^{-15}$
\end_inset

 for the remainder of this discussion.
 We also look at only at an example with 
\begin_inset Formula $d=2$
\end_inset

, i.e.
 
\begin_inset Formula $x\in\mathbb{R}^{2}$
\end_inset

 so the finite difference gradient function 
\begin_inset Formula $\nabla f_{N}:\mathbb{R}^{2}\rightarrow\mathbb{R}$
\end_inset

 is:
\begin_inset Formula 
\[
\nabla f_{N}(x)=[\frac{f_{N}([x_{1}+\epsilon,x_{2}])-f_{N}(x)}{\epsilon},\frac{f_{N}([x_{1},x_{2}+\epsilon])-f_{N}(x)}{\epsilon}]
\]

\end_inset

This works by calculating the slope after a small perterbation to 
\begin_inset Formula $x_{1}$
\end_inset

 and then again a small perterbation to 
\begin_inset Formula $x_{2}$
\end_inset

.
 The code implementation of this is on line 4 in Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "alg:minibatch-sgd"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
(b) (i) Gradient Descent with constant step-size
\end_layout

\begin_layout Standard
Several runs of gradient descent with various constant step-sizes, 
\begin_inset Formula $\alpha$
\end_inset

, are presented in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gd-constant"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The starting estimate is 
\begin_inset Formula $x=[3,3]$
\end_inset

.
 The function 
\begin_inset Formula $f_{T}(x)$
\end_inset

 has a local minimum at about 
\begin_inset Formula $x=[1,1]$
\end_inset

, but the global minimum is somewhere around 
\begin_inset Formula $x=[-1,-3]$
\end_inset

.
 A careless choice of 
\begin_inset Formula $\alpha$
\end_inset

 results in converging to the suboptimal local minimum, e.g.
 
\begin_inset Formula $\alpha=0.01$
\end_inset

.
 Either of 
\begin_inset Formula $\alpha=0.72$
\end_inset

 or 
\begin_inset Formula $\alpha=0.5$
\end_inset

 are reasonabl choices.
 For 
\begin_inset Formula $\alpha>0.72$
\end_inset

 we see divergence.
 The lowest value of 
\begin_inset Formula $F_{T}(x)$
\end_inset

 for the 
\begin_inset Formula $\alpha=0.72$
\end_inset

 run is marginally better than for the 
\begin_inset Formula $\alpha=0.5$
\end_inset

 run, however, the 
\begin_inset Formula $\alpha=0.5$
\end_inset

 converges marginally faster.
 For later experiments, where we are using Stochastic Gradient Descent,
 the noise caused by mini-batches will mean that we could diverge with lower
 values of 
\begin_inset Formula $\alpha$
\end_inset

, so from here on we select 
\begin_inset Formula $\alpha=0.5$
\end_inset

 rather than 
\begin_inset Formula $\alpha=0.72$
\end_inset

 to mitigate the risk of divergence.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bi.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:gd-constant"

\end_inset

Visualizations of gradient descent on 
\begin_inset Formula $f_{T}(x)$
\end_inset

 using a constant step size 
\begin_inset Formula $\alpha$
\end_inset

.
 On the left the function value is plotted against the iteration number.
 On the write the successive 
\begin_inset Formula $x_{t}$
\end_inset

s are plotted on a contour plot of 
\begin_inset Formula $f_{T}(x)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
(b) (ii) Stochastic Gradient Descent with constant batch-size and step-size
\end_layout

\begin_layout Standard
4 runs of Stochastic Gradient Descent with constant batch-size, 5, and constant
 
\begin_inset Formula $\alpha=0.5$
\end_inset

 are presented in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bii-sgd-constant"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Each run is different due to the randomness introduced by sampling the
 batches.
 The first step for three of the runs is broadly in the direction of the
 global minimum, but the first run, run 0, has a first step which is about
 
\begin_inset Formula $70^{\circ}$
\end_inset

 off.
 Nonetheless, this run's overall trajectory is very similar to the those
 of the others.
 Even when a run achieves an 
\begin_inset Formula $x_{t}$
\end_inset

 that minimizes 
\begin_inset Formula $f_{T}(.)$
\end_inset

 the run continues to `explore' a region due to the noise introduced by
 mini-batches, i.e.
 the gradients are like a random variables drawn from 
\begin_inset Formula $U(0,k)$
\end_inset

.
 However, the further the current estimate is from the a local minimum,
 the more coherent are the gradients 
\begin_inset Formula $\nabla f_{N_{t}}$
\end_inset

, 
\begin_inset Formula $\nabla f_{N_{t+1}}$
\end_inset

, etc., where 
\begin_inset Formula $N_{t}$
\end_inset

, 
\begin_inset Formula $N_{t+1}$
\end_inset

 etc.
 are different mini-batches.
 This greater coherence at a distance is what allows the algorithm converge
 to similar values on most runs.
 It is possible for the algorithm to diverge with the same hyper parameters,
 
\begin_inset Formula $\alpha=0.5$
\end_inset

 and mini-batch size of 5, but that did occur for any of the 4 runs presented
 here.
 The gradient descent runs reported above in 
\series bold
(b) (ii)
\series default
 have no randomness, so using 
\begin_inset Formula $\alpha=0.5$
\end_inset

 gives the same results every time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/bii.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:bii-sgd-constant"

\end_inset

Visualizations of gradient descent on 
\begin_inset Formula $f_{N_{t}}(x)$
\end_inset

 using a constant step size 
\begin_inset Formula $\alpha$
\end_inset

.
 
\begin_inset Formula $N_{t}$
\end_inset

 is drawn from 
\begin_inset Formula $T$
\end_inset

 by first shuffling 
\begin_inset Formula $T$
\end_inset

 and slicing 
\begin_inset Formula $T$
\end_inset

 into batches of size 5.
 On the left the function value is plotted against the iteration number.
 On the write the successive 
\begin_inset Formula $x_{t}$
\end_inset

s are plotted on a contour plot of 
\begin_inset Formula $f_{T}(x)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
