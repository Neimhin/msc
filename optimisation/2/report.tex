%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.


\documentclass[10pt]{article}
\usepackage{helvet}
\usepackage{caption}
\usepackage{float}
\usepackage{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=4cm,lmargin=2cm,rmargin=2cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage[backend=bibtex,maxbibnames=99]{biblatex}
\addbibresource{main.bib}

\floatstyle{plaintop}
\newfloat{listing}{htbp}{lop}
\floatname{listing}{Listing}

\definecolor{lightgray}{gray}{0.9} % Defines the color 'lightgray' with a specific shade.

\newmdenv[
  linecolor=lightgray,
  backgroundcolor=white,
  outerlinewidth=1pt,
  roundcorner=2pt,
  innertopmargin=10pt,
  innerbottommargin=10pt,
  innerrightmargin=10pt,
  innerleftmargin=10pt
]{greyborder}


\makeatletter

\DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tcolorbox}
\usepackage{amsthm}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{accents}
\usepackage{titlesec}
\usepackage{marginnote}
% \titleformat{\section}[block]{\normalfont\bfseries}{}{0em}{}


\usepackage{enumitem}
\usepackage{comment}
% \setlist{nolistsep}

\usepackage{tcolorbox}
\definecolor{light-blue}{cmyk}{0.24, 0.12, 0.0, 0.04, 1.00}
% \titlespacing\section{0pt}{0pt}{0pt}
% \titlespacing\subsection{0pt}{0pt}{0pt}
% \titlespacing\subsubsection{0pt}{0pt}{0pt}

\setlength{\headheight}{40pt}

\makeatother

\title{Week 2 Optimisation for Machine Learning}
\author{Neimhin Robinson Gunning, 16321701}
\date{\today}

\begin{document}
\maketitle
\lhead{Neimhin Robinson Gunning} \rhead{CS7CS4 Week 2 Assignment}

\section{Part (a)}
\subsection{Part (a) (i)}
% [x] TODO: Using sympy obtain an expression for the derivative dy/dx. Show all the steps that you used to obtain the expression.
To represent the expression $x^4$ using sympy we first create a symbol object for $x$ (line 1 in Listing~\ref{lst:ai-code}), and then raise it to the fourth power using standard python arithmetic notation. The \texttt{**} operation is overloaded such that \texttt{x**4} yields an object representing the expression $x^4$. Letting $y(x)=x^4$ the derivative $dy/dx$ is found with \texttt{(x**4).diff()}. Running the code we find this derivative to be $4x^3$.

\begin{listing}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{fig/ai-code.pdf}
  \end{center}
  \caption{Source code to find the derivative of $x^4$ using sympy. The result is $4x^3$.}\label{lst:ai-code}
\end{listing}

\subsection{Part (a) (ii)}
% [x] TODO: For a range of values of x calculate the derivative value using the expression from
% (i). Also calculate the derivative value estimated using a finite difference with
% perturbation δ = 0.01 on x. Plot these values on the same plot and compare.
% - [x] Show the code that you used to calculate the finite difference estimate.
The analytic derivative of $x^4$ is compared, for a range of values of $x$, to the finite difference method
approximations with a range of perterbations $\delta$ in Figure~\ref{fig:finite-diff-perterbations}.
The analytic derivative and the finite difference approximation with $\delta=0.01$ are different.
The function used to compute the finite difference approximation is presented in Listing~\ref{lst:finite-diff}.
It takes a sympy function \texttt{f}, a specific $x$ value \texttt{xval}, and a perterbation to compute the finite difference
according to Equation~\ref{eq:finite-diff}.

\begin{equation}
  \hat{\frac{dy}{dx}}(x)=\frac{(x + \delta)^4 - x^4}{\delta}
  \label{eq:finite-diff}
\end{equation}

\begin{listing}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/finite_diff_eg.pdf}
  \end{center}
  \caption{Ellided source code to estimate the derivative of $x^4$ using the finite difference method. The function \texttt{diff\_with\_pert} can be used to estimate the derivative of an arbitrary function of $x$, such that $f(x+\delta)$ and $f(x)$ are defined. The full script is in the appendix and named \texttt{src/aii.py}.}
  \label{lst:finite-diff}
\end{listing}

\subsection{Part (a) (iii)}
% [x] TODO: Now vary the size of the perturbation δ of x in the finite difference approximation
% from 0.001 to 1, and plot the difference between the derivative estimate and the
% exact derivative values caclulated using the sympy expression. Use a single plot
% here so that the effect of changing δ is easier to compare. How does changing δ
% affect the accuracy of the estimated derivative? Why?
The plot in Figure~\ref{fig:finite-diff-perterbations} shows the range $0.001\le x\le 0.0011$,
and we shows that the error between
the finite difference approximation and the analytic solution increases as the perterbation $\delta$ increases.
Perterbations in $[0.001,0.1]$ are shown\footnote{Including perterbations in $[0.001,1]$ would make the plot less readable so a narrower range was chosen}.
The relationship between the error and $\delta$ is not linear, and furthermore the relationship
depends on the value of $x$, though this dependence is not visible at the scale in the plot.

The accuracy of the finite difference approximation of the derivative is greater for smaller values of $\delta$. The reason for this
is related to the definition of the derivative. The derivative
is defined as $$\lim_{\delta\rightarrow 0}\frac{f(x+\delta)-f(x)}{x+\delta-x}$$, i.e. the derivative is the function approached by this expression as $\delta$ approaches 0. As we decrease $\delta$ and apply the finite difference method we are approaching the true derivative.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/finite-diff.pdf}
  \end{center}
  \caption{A comparison of the analytic derivation of $dy/dx=4x^3$ and some approximations of $dy/dx$ using the finite difference method.}\label{fig:finite-diff-perterbations}
\end{figure}


\section{(b)}
\subsection{(b) (i)}
% [x] TODO: Write a short python programme that implements gradient descent with a fixed step size α. Explain your code.
Listing \ref{lst:gradient-descent} shows ellided source code for a python class that can be used to run gradient descent.
Ellided functions allow for specification of the function to be optimized, the convergence condition, the max number of iterations etc.
Line 13 calculates the gradient at the current value $x_{n-1}$, and line 14 updates the value based on the gradient and step size, $x_n:=x_{n-1}-dy/dx(x_{n-1})\alpha$. The convergence condition is a function of $x_n$ and $x_{n-1}$. Examples of this class in use are in the appendix, e.g. in \texttt{src/bi.py}, in which the gradient function is computed with \texttt{sympy}.

\begin{listing}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{tmp/gradient_descent_listing.pdf}
  \end{center}
  \caption{Ellided source code a python class that implements gradient descent with a fixed step size.}\label{fig:ai-code}
  \label{lst:gradient-descent}
\end{listing}


\subsection{(b) (ii)}
% [x] TODO: With initial value x = 1 and step sizes α = 0.1 run your gradient descent
% programme with the function y = x4 . Plot how x and y(x) vary with each
% gradient descent iteration. Discuss.

In Figure \ref{fig:gradient-descent-bi} is a plot of gradient descent on $x^4$ with $x_0=1$ and $\alpha=0.1$.
We see that updates to $|\hat x|$ and $y(\hat x)$ are initially large but quickly become tiny.
Because $x^4$ is very flat around the minimum (see subplot (c)) we have a very small gradient, and thus very small step sizes,
so the algorithm updates extremely slowly in the flat region of $x^4$, i.e. near $\hat x=0$,
but cannot reach it $\hat x=0$ in a finite number of iterations.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/gradient-descent-bi.pdf}
  \end{center}
  \caption{Running gradient descent on $x^4$ with initial value $x_0=1$ and step size $\alpha=0.1$. 
  The yellow line in subplot (c) is $x^4$. Note the log scales in (a) and (b).}\label{fig:gradient-descent-bi}
\end{figure}

\subsection{(b) (iii)}
% [ ] TODO: Now repeat (ii) for a range of initial values for x and step sizes α. Be sure to
% choose a large enough range of step sizes that you see a variety of behaviours
% (including non-convergence)
% How does the choice of alpha affect the convergence time? Why?
In Figure \ref{fig:gradient-descent-b1} several runs of gradient descent on
$x^4$ are visualized. Subplot (a) shows $|\hat{x}|$ for each iteration, which is the distance of the estimate $\hat{x}$ from
the optimal value, $x=0$. With initial value $x=1$ and step size $\alpha=0.1$ we see that the steps are initially large, i.e. moving quickly to the optimum, but on successive iterations the steps slow down dramatically. In subplot (b) we see the value of $y(\hat{x})$ on each iteration.
  
The convergence criterion for gradient descent adopted here
is when $|\alpha\Delta x|<0.001$ or $x$ is $\pm\infty$\footnote{Meaning the floating point values \texttt{math.inf} or \texttt{-math.inf}}.
For the function $x^4$ we can see from Table \ref{tab:alpha-start-convergence} that the final guess, i.e. the estimate of $\argmin x^4$ is
most accurate and converges fastest with larger values of $\alpha$, \textbf{unless} $\alpha$ is too large and the iterations of gradient descent degenerate, i.e. subsequent iterations give worse or unchanged estimates.
The reason for this is that the gradient of $x^4$ is tiny as $x$ approaches the minimum. So, a smaller value of $\alpha$ means the convergence condition, $|\alpha\Delta x|<0.001$, can be met with a higher value of $\Delta x$, but in the case of $x^4$, $\Delta x$ is higher when $x$ is further from the optimal value 0. In other words a lower $\alpha$ allows stopping further from the minimum. This is why the final guess
is worse for smaller values of $\alpha$, but why does a smaller
$\alpha$ lead to slower convergence time?

Since $\alpha$ is constant for a single run of gradient descent, the convergence condition depends on $\Delta x$, namely we need $|\Delta x|<0.001/\alpha$. A larger $\alpha$ entails a larger step, but there
are two factors at play here:
\begin{enumerate}
  \item A larger step size may entail a smaller $\Delta x$ on the subsequent iteration.
  \item A larger step size moves us away from meeting the convergence condition.
\end{enumerate}
It turns out for $x^4$ that the first of these factors is more important, in cases where gradient descent is converging. Thus larger $\alpha$ results in faster convergence (for $x^4$).

Similarly, the initial value of $x$ turns out to be relatively less important, assuming the configuration converges.
The difference in progression between starting with any of $x=0.7,1,1.5$, while holding $\alpha=0.1$, is small, as seen in Figure~\ref{fig:gradient-descent-b1}. The algorithm needs a handful of extra iterations when $x_0$ is larger, but arrives at the same result and follows a similar trajectory.

As we increase $\alpha$ there are two important points at which behaviour changes.
The first point is the perfect setting of $\alpha$ such that the minimum is found after one iteration.
Above this point the iterations of gradient descent will chatter, but still converge, up until the second
important point, where the first iteration results in a worse estimate.
After this point the algorithm does not converge. (In the case of $x^4$).

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/gradient-descent-b1.pdf}
  \end{center}
  \caption{Various runs of gradient descent on $x^4$. One run, where $\alpha=0.5$ and $x_0=1$, is stuck in a 2-step loop, flipping $\hat x$ back and forth between 1 and -1.
  The yellow line in subplot (c) is $x^4$.}\label{fig:gradient-descent-b1}
\end{figure}

\begin{table}
  \caption{Various runs of gradient descent on $x^4$ with different outcomes. The maximum number of iterations was set to 100, and the convergence threshold to $0.001$. Some of the $\alpha$ values were purposefully selected to lie near the margin of non-convergence, which can be calculated with $\alpha_m=\frac{2x_0}{4x_0^3}$. So rows 8 and 9 use $\alpha_m\pm\epsilon$.}
  \label{tab:alpha-start-convergence}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/gradient-descent-b2.csv.pdf}
  \end{center}
\end{table}

\section{(c)}
\subsection{(c) (i) Optimising $\gamma x^2$}
% [ ] TODO How does changing \gamma affect the convergence of your gradient descent program? Why?
Firstly, note that if $\gamma$ is negative, then $\gamma x^2$ and $\gamma |x|$ have no
minimum, and thus gradient descent will behave weirdly. For instance, in my implementation,
with enough iterations it will actually finish with \texttt{math.inf}, which is reasonable.
If $\gamma=0$ then gradient descent terminates immediately, and any value for $x$ minimizes $\gamma x^2=0x^2$.
Increasing $\gamma$ increases the magnitude of the gradient, resulting in larger steps,
but the convergence time and accuracy will then depend on the initial value.

Several runs of gradient descent on $\gamma x^2$ are visualized in Figure~\ref{fig:gradient-descent-ci}.

Let $d_0=|x_0-\argmin_x\gamma x^2|$ be the initial distance to the optimal value of $x$.
Let's assume $x_0$ is `far' from $$\argmin_x\gamma x^2$$, i.e. $d_0$ is large,
meaning gradient descent will at first take several steps in the same direction.
Increasing $\gamma$ to $\gamma'$, i.e. $\gamma'>\gamma$, results in a larger gradient $2\gamma' x'_0 > 2\gamma x_0$
meaning the first step is larger.
If after the first iteration the distance has improved, $d_1 < d_0$ then gradient descent will converge, if not it will diverge.

Let's assume we are in a converging condition.
While the second gradient $2\gamma' x'_1$ is not necessarily larger than $2\gamma x_1$, the updated $x'_1$ is now closer to the minimum than
$x_1$. The result of these dynamics is that with a larger $\gamma'$ there is
a fewer number iterations until the first `overshoot'.
This means that convergence time is smaller for the larger $\gamma'$ than for $\gamma$.
Also, for fixed $x$ and $\alpha$, the step size is greater when $\gamma$ is greater which means we can move closer to
the minimum $x=0$ without triggering the convergence condition, $\alpha\frac{dy}{dx}(x)\le\epsilon$.
Thus, both the convergence time and the final estimate are better when $\gamma$ is higher, assuming a convergence condition.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/gradient-descent-ci.pdf}
  \end{center}
  \caption{
    Various runs of gradient descent on $\gamma x^2$.
    Subplot (a) shows the estimate of $x$ for each iteration.
    The yellow lines in subplot (c) are plots of the various $\gamma x^2$ functions.
  }\label{fig:gradient-descent-ci}
\end{figure}

\subsection{(c) (ii) Optimising $\gamma |x|$}
% [x] TODO How does changing \gamma affect the convergence of your gradient descent program? Why?
Here we assume $\gamma>0$.
In Figure~\ref{fig:gradient-descent-cii} several runs of gradient descent on $\gamma |x|$ are plotted.
The magnitude of the gradient of $\gamma |x|$ is constant for all $x\neq0$, $dy/dx=\gamma$.
The function $\gamma|x|$ is not smooth because it is not differentiable at $x=0$.
Here we assume $\frac{d\gamma |x|}{dx}(0)=0$.
The only cases where we see convergence are when the algorithm lands exactly on $x=0$, because
the step size is constant, so when there's an overshoot the next iteration will revert back to the previous estimate.
In all other cases there will be ``chatter'' once the estimate is close enough to the minimum, namely $|\hat x|<\alpha\gamma$.
At this point the algorithm will flip between two estimates repeatedly until \texttt{max\_iter}.

Therefore, rather than discuss convergence time as defined previously
we can consider the number of iterations
until the algorithm loops.
For fixed $x_0$ and $\alpha$ the time to first loop is negatively correlated with $\gamma$, because
a larger $\gamma$ means larger steps.
The error of the final estimate (given sufficient iterations), $d$, will satisfy
$d<\alpha\gamma$, thus a smaller $\gamma$ entails a smaller maximum error.
If we tweaked the algorithm to record and return the best choice of $x$ for the whole run
then the maximum error would be $\frac{1}{2}\alpha\gamma$.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{fig/gradient-descent-cii.pdf}
  \end{center}
  \caption{
    Various runs of gradient descent on $\gamma |x|$.
    Subplot (a) shows the estimate of $x$ for each iteration.
    The yellow lines in subplot (c) are plots of the various $\gamma |x|$ functions.
  }\label{fig:gradient-descent-cii}
\end{figure}

\bigskip{}
\printbibliography
\end{document}
