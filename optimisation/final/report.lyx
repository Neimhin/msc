#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection*
1.
 Evaluating regularisation effect of mini-batch size
\end_layout

\begin_layout Standard
Here we attempt to shed some light on the potential regularisation effect
 of different mini-batch sizes when doing stochastic gradient descent.
 In particular we are interested in whether the gradient ``noise'' that
 occurs with smaller mini-batch sizes can help to reduce overfitting and/or
 improve generalisation performance.
\end_layout

\begin_layout Standard
We focus on the CIFAR10 task, and use a CNN model.
 The code for the precise CNN model is available in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{src/baseline.py}
\end_layout

\end_inset

, and for brevity is not described in detail here.
 The important thing about the model is that in 
\series bold
can 
\series default
overfit on the CIFAR10 task.
\end_layout

\begin_layout Standard
To expedite the following experiments we select a low Dropout such that
 we can see the effects of overfitting with a smaller number of epochs.
\end_layout

\begin_layout Section*
2.
\end_layout

\begin_layout Subsection*
(a) Line Search
\end_layout

\begin_layout Standard
In gradient descent we update the parameters of our model based on the gradient
 at the current estimate 
\begin_inset Formula $x_{t}$
\end_inset

, using:
\begin_inset Formula 
\[
x_{t+1}=x_{t}-\alpha\nabla f(x_{t})
\]

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 has to be selected.
 Exact line search is an approach where 
\begin_inset Formula $\alpha$
\end_inset

 is selected to exactly minimize 
\begin_inset Formula $f(x_{t}-\alpha\nabla f(x_{t}))$
\end_inset

.
 Exact line search is not necessarily practical because finding 
\begin_inset Formula $\alpha\in\text{arg min}_{\alpha'}f(x_{t}-\alpha'\nabla f(x_{t}))$
\end_inset

 is itself an optimisation problem.
\end_layout

\begin_layout Standard
Instead we can use grid search to select 
\begin_inset Formula $\alpha$
\end_inset

 at each iteration.
 This works by testing a set of possible set of values for 
\begin_inset Formula $\alpha^{g}\in a_{1},a_{2},\ldots,a_{m}$
\end_inset

, at each step and selecting the one that minimizes 
\begin_inset Formula $f(x_{t}-\alpha^{g}\nabla f(x_{t}))$
\end_inset

.
 This means we can control the number of function evaluations performed
 for each grid search (
\begin_inset Formula $m$
\end_inset

).
\end_layout

\begin_layout Standard
When running gradient descent without line search the choice of step size
 
\begin_inset Formula $\alpha$
\end_inset

 is crucial to whether and how quickly the algorithm converges.
 One advantage of line search is that it can automatically select a reasonable
 
\begin_inset Formula $\alpha$
\end_inset

 at each step.
 For one this means there may be one fewer hyperparameter to tune (so long
 as we don't introduce more hyperparameters in the line search implementation),
 and it means that the step size is adaptive, allowing the chosen step size
 to automatically shrink or grow on successive iterations as appropriate.
\end_layout

\begin_layout Standard
A disadvantage of line search is that calculating or estimating 
\begin_inset Formula $\alpha\in\text{arg min}_{\alpha'}f(x_{t}-\alpha'\nabla f(x_{t}))$
\end_inset

 may be computationally expensive, so even if line search improves convergence
 time w.r.t.
 number of iterations, it might not improve convergence time w.r.t.
 wall-clock time.
\end_layout

\begin_layout Standard
Whether line search should be used alongside Stochastic Gradient Descent
 is, of course, situation dependent.
 When function evaluations are extremely costly it may simply be prohibitively
 expensive to run grid-search line-search or backtracking line-search.
 In Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:twoa"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we compare two runs of SGD with constant step 
\begin_inset Formula $\alpha=0.001$
\end_inset

 and 
\begin_inset Formula $\alpha=0.3$
\end_inset

 against one run of SGD with grid-search line-search, 
\begin_inset Formula $\alpha\in[0.00001,0.0001,0.001,0.01,0.1,1,10]$
\end_inset

.
 With 
\begin_inset Formula $\alpha=0.001$
\end_inset

 the constant alg gets stuck in a local minimum, and also progresses very
 slowly anyway.
 The line-search algorithm doesn't get stuck at the local minimum.
 However, constant step size 
\begin_inset Formula $\alpha=0.3$
\end_inset

 converges in fewer iterations (with each iteration being cheaper than for
 line-search).
 So, while grid search line search helps avoid a local minimum pitfall in
 this case, we can achieve the same by tuning the hyperparameter in normal
 gradient descent.
\end_layout

\begin_layout Standard
In summary, grid-search line-search, and presumably other types of line-search,
 
\series bold
can
\series default
 be used with SGD, but whether this approach yields advantages over hyperparamet
er tuning is a matter for empirical investigation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig/twoa.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:twoa"

\end_inset

Stochastic Gradient Descent using `constant' and `line-search' step size
 methods.
 The dataset is simulated and has size 25.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
